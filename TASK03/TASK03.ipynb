{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvJ2OJedCH3D"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from textblob import TextBlob\n",
        "from wordcloud import WordCloud"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#configure plotting\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.rcParams[\"figure.figsize\"] = (10, 6)"
      ],
      "metadata": {
        "id": "Y_a_pWcGCVA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the dataset\n",
        "df1 = pd.read_csv('/student_feedback.csv')\n",
        "df2 = pd.read_csv('/Student_Satisfaction_Survey.csv', encoding='ISO-8859-1')"
      ],
      "metadata": {
        "id": "6kStfatbCVEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.columns"
      ],
      "metadata": {
        "id": "9gEJ499YGmTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "cleaning dataset1\n",
        "\n",
        "\n",
        "What is Unnamed: 0? When a CSV file is saved from tools like Excel or pandas with index=True (the default), the row numbers (indexes) are also saved as a column.\n",
        "This creates an unnecessary column named \"Unnamed: 0\" when the CSV is read again, as the index is already automatically handled by pandas.\n",
        "What does drop(..., inplace=True) do? drop(columns=['Unnamed: 0']) tells pandas to remove the specified column.\n",
        "inplace=True ensures the operation modifies df1 directly, without creating a new copy.\n"
      ],
      "metadata": {
        "id": "2AO8BC5OISeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean df1: Drop unnecessary columns\n",
        "df1.drop(columns=['Unnamed: 0'], inplace=True)"
      ],
      "metadata": {
        "id": "04VW9eRSISDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.columns"
      ],
      "metadata": {
        "id": "OZo6WENSHIN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**cleaning dataset 2**\n",
        "\n",
        "\n",
        "This removes any leading or trailing spaces from column names in df2.\n",
        "\n",
        "Why it's important:\n",
        "\n",
        "Sometimes, CSV or Excel files have hidden whitespace in column names, causing bugs like:\n",
        "\n",
        "KeyError: 'Computed_Average' (even if it looks like the column exists)\n",
        "\n",
        "Problems during merging, plotting, or filtering."
      ],
      "metadata": {
        "id": "SjIHgBD-Ijz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean df2: Clean column names\n",
        "df2.columns = df2.columns.str.strip()"
      ],
      "metadata": {
        "id": "Faa-pB4iIfMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate average satisfaction scores from df1\n",
        "rating_columns = df1.columns.drop('Student ID')\n",
        "df1['Average_Score'] = df1[rating_columns].mean(axis=1)"
      ],
      "metadata": {
        "id": "311p0-l8IfOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "CHART -1 HISTOGRAM OF NO OF STUDENTS & AVG SCORE"
      ],
      "metadata": {
        "id": "pHG5vWAoI4O5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot: Overall satisfaction score distribution (df1)\n",
        "sns.histplot(df1['Average_Score'], bins=10, kde=True)\n",
        "plt.title('Distribution of Average Satisfaction Scores')\n",
        "plt.xlabel('Average Score')\n",
        "plt.ylabel('Number of Students')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zidH9VzYI7R5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The histogram displays the distribution of average satisfaction scores among students.\n",
        "\n",
        "The x-axis shows the average score ranges, divided into 10 bins.\n",
        "\n",
        "The y-axis shows the number of students who fall into each score range.\n",
        "\n",
        "The KDE (smooth curve) shows the overall trend or shape of the score distribution.\n",
        "\n",
        "Helps identify whether most students gave high, medium, or low ratings."
      ],
      "metadata": {
        "id": "PgI2uaUwI3Ii"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chart 2:- Bar plot score and question"
      ],
      "metadata": {
        "id": "UQZWZwsOJEPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Plot: Average score per question (df1)\n",
        "avg_per_question = df1[rating_columns].mean().sort_values()\n",
        "\n",
        "sns.barplot(\n",
        "    x=avg_per_question.values,\n",
        "    y=avg_per_question.index,\n",
        "    hue=avg_per_question.index,  # setting hue\n",
        "    palette='viridis',\n",
        "    dodge=False,\n",
        "    legend=False\n",
        ")\n",
        "\n",
        "plt.title('Average Score per Question')\n",
        "plt.xlabel('Average Score')\n",
        "plt.ylabel('Question')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EBLbJAarJAXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Well versed with the subject\" received the highest average score, reflecting strong subject knowledge.\n",
        "\n",
        "\"Explains concepts clearly\" and \"Use of presentations\" also scored well, indicating effective teaching methods.\n",
        "\n",
        "Lower scores for \"Solves doubts willingly\" and \"Difficulty of assignments\" suggest areas needing improvement in student support and assignment clarity.\n",
        "\n",
        "First, the total number of responses for each question is calculated by adding up how many students gave ratings from 1 to 5.\n",
        "\n",
        "Then, the weighted average score is computed by multiplying each rating value (1 to 5) by the number of times it was given, summing those values, and dividing by the total number of responses.\n",
        "\n",
        "This gives a more accurate average that considers both the rating and how many students selected it"
      ],
      "metadata": {
        "id": "rHSbcwz-JSok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute computed average from df2 (weighted average)\n",
        "df2['Total_Responses'] = df2[[f'Weightage {i}' for i in range(1, 6)]].sum(axis=1)\n",
        "df2['Computed_Average'] = sum(df2[f'Weightage {i}'] * i for i in range(1, 6)) / df2['Total_Responses']"
      ],
      "metadata": {
        "id": "gzgwUvqyJTmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar plot of AVG rating per question"
      ],
      "metadata": {
        "id": "k9hyjefHJbQN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleans the data by replacing curly/smart apostrophes in the \"Questions\" column with regular apostrophes to avoid font or display issues.\n",
        "\n",
        "Sorts the questions based on their computed average scores in descending order (highest to lowest).\n",
        "\n",
        "Creates a horizontal bar chart where:\n",
        "\n",
        "The y-axis shows the questions.\n",
        "\n",
        "The x-axis shows the computed average scores.\n",
        "\n",
        "Each bar's color comes from the \"coolwarm\" color palette, and each question is used as a hue to apply different shades.\n",
        "\n",
        "The legend is turned off to avoid clutter since each question is already labeled on the y-axis.\n",
        "\n",
        "The chart shows how well each question scored on average, making it easy to compare satisfaction levels across different areas"
      ],
      "metadata": {
        "id": "S52YOyDDJbZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Average satisfaction by Question (df2)\n",
        "df2['Questions'] = df2['Questions'].str.replace('’', \"'\", regex=False)\n",
        "\n",
        "top_questions = df2[['Questions', 'Computed_Average']].sort_values(by='Computed_Average', ascending=False)\n",
        "\n",
        "sns.barplot(\n",
        "    data=top_questions,\n",
        "    x='Computed_Average',\n",
        "    y='Questions',\n",
        "    hue='Questions',\n",
        "    palette='coolwarm',\n",
        "    dodge=False,\n",
        "    legend=False\n",
        "    )\n",
        "\n",
        "plt.title('Average Rating per Question (Aggregated)')\n",
        "plt.xlabel('Average Score')\n",
        "plt.ylabel('Question')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Gczj9fCkJYeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "word plot of feedback question"
      ],
      "metadata": {
        "id": "3MAsL-h8Jrx6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Word Cloud: Top feedback questions (df2)\n",
        "text = ' '.join(df2['Questions'].dropna().tolist())\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)"
      ],
      "metadata": {
        "id": "QAZ5Ci9uJsiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Word Cloud of Feedback Questions\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aOZRioC0JyMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nSuggestions for Improvement:\")\n",
        "low_avg_questions = top_questions[top_questions['Computed_Average'] < 3.5]\n",
        "for _, row in low_avg_questions.iterrows():\n",
        "    print(f\"• {row['Questions']} → Avg Score: {row['Computed_Average']:.2f}\")"
      ],
      "metadata": {
        "id": "jtyaPXHjJ3iJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zLKu2xxuJ8Tz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}